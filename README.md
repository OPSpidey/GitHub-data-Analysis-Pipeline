# GitHub-data-Analysis-Pipeline
This project was developed to showcase a real-time, end-to-end data pipeline leveraging modern data engineering tools. The system ingests GitHub event data, streams it via Apache Kafka, processes it using Apache Spark, stores the transformed data in MySQL, and presents interactive visualizations through a custom React dashboard. All components are fully containerized with Docker and orchestrated using Terraform alongside LocalStack to simulate AWS services locally.

ðŸ“¦ Tech Stack

Docker & Docker Compose â€“ Containerization of all services

LocalStack â€“ Local AWS Cloud simulation (S3 integration)

Terraform â€“ Infrastructure as Code for resource provisioning

Apache Kafka â€“ Real-time data streaming

PySpark â€“ Big Data batch processing

MySQL â€“ Persistent relational storage

Flask â€“ Backend REST API for data access

React â€“ Frontend dashboard for visualization

ðŸ“Š Pipeline Workflow

Step	Description	File

1	GitHub data producer publishes events to Kafka	github_producer.py

2	Kafka consumer writes ingested data to S3	kafka_to_s3.py

3	Spark job processes S3 data and loads it into MySQL	spark_s3_to_mysql.py

âœ¨ Key Outcomes

Gained practical experience in Kafka-based event streaming, Spark transformations, and REST API integration

Built a containerized full-stack data pipeline using modern tools and Infrastructure as Code principles

Strengthened hands-on skills in data ingestion, processing, and visualization
